{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "glA3NV9MRl63"
   },
   "source": [
    "# DeepSORT - Lab 7.5\n",
    "\n",
    "## Recap\n",
    "This is the Lab using a CNN based algorithm, DeepSORT to track multiple people as part of CE6003's Object Tracking section. You should complete the tasks in this lab as part of the DeepSORT section of the lesson.\n",
    "\n",
    "Please remember this lab must be completed before taking the quiz at the end of this lesson.\n",
    "\n",
    "First, if we haven't already done so, we need to clone the various images and resources needed to run these labs into our workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VP1Dea5sKuEx"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This is an implementation, by Z. Q. Pei of a multiple object tracking algorithm, namely DeepSORT. DeepSORT is basically the same as SORT but with an added CNN model to extract features in image of human part bounded by a detector. \n",
    "\n",
    "This CNN model is a RE-ID model and the detector used in the paper documenting it, by Wokje et al, is FasterRCNN, and the original source code for DeepSORT is linked below.\n",
    "\n",
    "In the original code, the CNN model was implemented in tensorflow, but Z. Q. Pei re-implemented the CNN feature extraction model with PyTorch and this is the code we're working with today. \n",
    "\n",
    "Also, Z. Q. Pei used YOLOv3 to generate bboxes instead of FasterRCNN.\n",
    "\n",
    "It's an interesting relatively recent approach, leading to the intuition that we can effectively have a relatively low cost tracker which uses feature vectors extracted from later layers of detectors as the basis of a cost-function to either enhance or replace a traditional Kalman Filter based tracking approach.\n",
    "\n",
    "See the video accompanying this lab for a review of recent CNN-based tracking approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z8vQZvEMPIau"
   },
   "source": [
    "# Images and Code\n",
    "\n",
    "First, check out the DeepSORT algorithm etc and a video to run the demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['http_proxy'] = 'http://proxy-chain.intel.com:911'\n",
    "os.environ['HTTP_PROXY'] = 'http://proxy-chain.intel.com:911'\n",
    "os.environ['https_proxy'] = 'https://proxy-chain.intel.com:912'\n",
    "os.environ['HTTPS_PROXY'] = 'https://proxy-chain.intel.com:912'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "md1qGUkYR6H-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'deep_sort_pytorch'...\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/EmdaloTechnologies/CE6003.git\n",
    "#!git clone https://github.com/mcnamarad1971/CE6003.git\n",
    "\n",
    "# git clone a pytorch re-implementation of Wolke et al's SORT by ZQ Pei\n",
    "!git clone https://github.com/ZQPei/deep_sort_pytorch.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "szIcY5RkwI8L"
   },
   "source": [
    "**Install YOLO3 Weights**\n",
    "\n",
    "Download a set of YOLOv3 weights curated by Joseph Redmon.  DeepSORT will use YOLOv3 to do its detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0dVFZnigwPLb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!wget https://pjreddie.com/media/files/yolov3.weights\n",
    "#!cp /content/yolov3.weights /content/deep_sort_pytorch/YOLOv3\n",
    "os.system(\"wget https://pjreddie.com/media/files/yolov3.weights\")\n",
    "os.system(\"cp /content/yolov3.weights /content/deep_sort_pytorch/YOLOv3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nwkqW2mcwXKV"
   },
   "source": [
    "**DeepSort**\n",
    "\n",
    "Download DeepSORT Checkpoint ckpt.t7, curated by ZQ Pei. DeepSORT will use this Checkpoint to measure similarity between the objects in bounding boxes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eOcnU1fSwaW9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download cached copy of ZQ Pei's ckpt.t7 checkpoint for deepsort\n",
    "#!cp /content/CE6003/code/ckpt.t7 /content/deep_sort_pytorch/deep_sort/deep/checkpoint\n",
    "os.system(\"cp /content/CE6003/code/ckpt.t7 /content/deep_sort_pytorch/deep_sort/deep/checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L3PrxNw-wsYx"
   },
   "source": [
    "Now, let's run the default demo over our ```pedestrians.mp4``` video.  Now that we have supplied weights for Yolo and parameters for Deep SORT the defaults are sufficient for us today.\n",
    "\n",
    "```demo_yolo3_deepsort.py VIDEO_PATH```\n",
    "\n",
    "$\\qquad$ ```[--help] ```\n",
    "\n",
    "$\\qquad$ ```[--yolo_cfg YOLO_CFG]```\n",
    "\n",
    "$\\qquad$ ```[--yolo_weights YOLO_WEIGHTS]```\n",
    "\n",
    "$\\qquad$ ```[--yolo_names YOLO_NAMES]```\n",
    "\n",
    "$\\qquad$ ```[--conf_thresh CONF_THRESH]```\n",
    "\n",
    "$\\qquad$ ```[--nms_thresh NMS_THRESH]```\n",
    "\n",
    "$\\qquad$ ```[--deepsort_checkpoint DEEPSORT_CHECKPOINT]```\n",
    "\n",
    "$\\qquad$ ```[--max_dist MAX_DIST]```\n",
    "\n",
    "$\\qquad$ ```[--ignore_display]```\n",
    "\n",
    "$\\qquad$ ```[--display_width DISPLAY_WIDTH]```\n",
    "\n",
    "$\\qquad$ ```[--display_height DISPLAY_HEIGHT]```\n",
    "\n",
    "$\\qquad$ ```[--save_path SAVE_PATH]```\n",
    "\n",
    "$\\qquad$ ```[--use_cuda USE_CUDA]```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I12nlxUXwxfL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!cd /content/deep_sort_pytorch && python demo_yolo3_deepsort.py --use_cuda=False --ignore_display --save_path=/content/video.mp4 /content/CE6003/images/lab7/vids/pedestrians.mp4\n",
    "os.system(\"cd /content/deep_sort_pytorch && python demo_yolo3_deepsort.py --use_cuda=False --ignore_display --save_path=/content/video.mp4 /content/CE6003/images/lab7/vids/pedestrians.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8fZJvZXeY15y"
   },
   "source": [
    "**Video**\n",
    "\n",
    "Thia code plays the video we just created.\n",
    "\n",
    "As you can see, adding the concept of tracking features in a more human-like manner, by adding significant weight to the intuition that two detections - one in previous frame and one in current frame - look similar to each other significantly boosts a tracker's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mxvG8It3RlWy"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import base64\n",
    "import time\n",
    "from IPython.display import clear_output, Image, display\n",
    "#from google.colab.patches import cv2_imshow\n",
    "\n",
    "def arrayShow (imageArray):\n",
    "    ret, png = cv2.imencode('.png', imageArray)\n",
    "    encoded = base64.b64encode(png)\n",
    "    return Image(data=encoded.decode('ascii'))\n",
    "\n",
    "video = cv2.VideoCapture(\"video.mp4\")\n",
    "while(video.isOpened()):\n",
    "        clear_output(wait=True)\n",
    "        ret, frame = video.read()\n",
    "        if(ret == False):\n",
    "          break\n",
    "        lines, columns, _ =  frame.shape\n",
    "        img = arrayShow(frame)\n",
    "        display(img)\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OH4IGJkZlO_u"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "## Exercises\n",
    "**Project 1**\n",
    "This may form the basis of a reasonably sized project.  DeepSORT uses a second CNN to determine association.  It may be interesting to review the original DeepSORT / SORT papers etc and determine if an approach that sends feature descriptors from late layers in detectors such as YOLOv3, SSD, FasterRCNN along with bounding boxes combined with a Hungarian/Kalman has been thoroughly explored already or if there's scope for novel work.\n",
    "\n",
    "This is not a requirement for completing this lab!\n",
    "\n",
    "\n",
    "## Takeaways\n",
    "1. You've seen evidence that tracking objects based on what they look like instead of based on models of their motion etc. are very powerful.\n",
    "2. You've seen an example of such an algorithm in play.\n",
    "3. You've an intuition that there seems to be a lower computational cost approach possible - where features are delivered with bounding boxes for assignment via auction cost algorithm.\n",
    "4. You might be interested in undertaking a project to determine how effective that approach is.\n",
    "\n",
    "## References\n",
    "* ZQ Pei pytorch variant of Deep SORT https://github.com/ZQPei/deep_sort_pytorch\n",
    "* Wojke et al, DeepSORT paper, https://arxiv.org/pdf/1703.07402.pdf\n",
    "* Wojke et al, DeepSORT code, https://github.com/nwojke/deep_sort\n",
    "* Redmon et al, YOLO3 paper, https://pjreddie.com/media/files/papers/YOLOv3.pdf\n",
    "* Redmon et al, YOLOv3 code, https://pjreddie.com/darknet/yolo/\n",
    "* Bewley et al, SORT code, https://github.com/abewley/sort\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_7_6_DeepSort.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
