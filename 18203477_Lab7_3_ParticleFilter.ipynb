{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "18203477_Lab7_3_ParticleFilter.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glA3NV9MRl63",
        "colab_type": "text"
      },
      "source": [
        "# Particle Filter - Lab 7.3\n",
        "\n",
        "## Recap\n",
        "This is the Lab on using a Particle Filter in CE6003's Object Tracking. You should complete the tasks in this lab as part of the Particle Filter section of the lesson.\n",
        "\n",
        "Please remember this lab must be completed before taking the quiz at the end of this lesson.\n",
        "\n",
        "First, if we haven't already done so, we need to clone the various images and resources needed to run these labs into our workspace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "md1qGUkYR6H-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/EmdaloTechnologies/CE6003.git\n",
        "#!git clone https://github.com/mcnamarad1971/CE6003.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvp_ecbWSB6V",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Program Description**\n",
        "\n",
        "This program demonstrates a very simple 'tracking' mechanism - derived from a Particle filter. We're going to use our Particle filter to track a single object, namely a person.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nlei6UBfjbbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(1, \"/content/CE6003/code\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWvUXFH1iaoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import io\n",
        "import cv2\n",
        "import time\n",
        "import numpy as np\n",
        "import base64\n",
        "from IPython.display import clear_output, Image, display\n",
        "from motion_tracking import ParticleFilter  # Import pre-cooked Particle filter\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GSU76keSNr7",
        "colab_type": "text"
      },
      "source": [
        "#The Story So Far\n",
        "\n",
        "To illustrate how to track something in a video stream, we have used the following technique to generate a set of images for you to work on.\n",
        "\n",
        "What we did was we generated a short video - just recording one person walking around, on an iPhone.\n",
        "\n",
        "Then we used ```ffmpeg``` to decompose about 7 seconds of that video down into still images.\n",
        "\n",
        "```ffmpeg -i $vid_in -vf fps=30 imgs/daire%03d.png```\n",
        "\n",
        "We saved those frames as ```imgs/daire%03d.png``` in the git repository in the single-detections directory\n",
        "\n",
        "We've run yolo3 over those frames to generate bounding boxes and saved those bounding boxes into the same directory.\n",
        "\n",
        "The file format is comma-separated values and the values are as shown here:\n",
        "\n",
        " frame index | object-type | centre-x | centre-y | width | height | confidence\n",
        " --- | --- | --- | --- | --- | --- | ---\n",
        " int | -1 | float | float | float | float | float\n",
        "\n",
        "* The object type is always a person - that's all we inferred for.\n",
        "* The centre-x and width are fractions of the image's width\n",
        "* The centre-y and height are fractions of the image's height\n",
        "* The confidence is supplied by Yolo3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYWphuuLX6hr",
        "colab_type": "text"
      },
      "source": [
        "*What Happens Now*\n",
        "\n",
        "For each image in the directory, in order,\n",
        "* we'll find the centre of the detection in that image (if any)\n",
        "* we'll build a bounding box for the detection in that image\n",
        "* we'' derive a variance term (crudely) from the Yolo confidence for that image\n",
        "* and we'll supply the centre of that bounding box along with the variance term to a Particle Filter implementation\n",
        "\n",
        "Then, we'll explore how a Particle filter tracks the object in the image stream."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY4bHjZsuWQ2",
        "colab_type": "text"
      },
      "source": [
        "**Key Parameters**\n",
        "\n",
        "We have four key parameters to shape our particle filter.  We have the number of particles, and three terms associated with an action estimate;\n",
        "xVel, yVel, and velStd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JujHzAd7iWo_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numParticles = 10\n",
        "xVel = 5\n",
        "yVel = 5\n",
        "velStd = 25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsQyMnFpZBUP",
        "colab_type": "text"
      },
      "source": [
        "**Get File Handles**\n",
        "\n",
        "This function gets the filenames of all the files in the directory, in a reproducible order, and loads in the bounding boxes from file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1lGqvyGZBjj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_pngs_and_boxes():\n",
        "    pngdir = \"/content/CE6003/images/lab7/single-objects/\"\n",
        "    bbdir = \"/content/CE6003/images/lab7/single-objects/\"\n",
        "\n",
        "    pngfolder = os.fsencode(pngdir)\n",
        "    bbfolder = os.fsencode(bbdir)\n",
        "\n",
        "    pngfiles = []\n",
        "    for filename in os.listdir(pngfolder):\n",
        "        if filename.decode().endswith(\".png\"):\n",
        "            pngfiles.append(pngdir + filename.decode())\n",
        "    pngfiles.sort()\n",
        "\n",
        "    for filename in os.listdir(bbfolder):\n",
        "        if filename.decode().endswith(\".boxes\"):\n",
        "            bbfilename = bbdir + filename.decode()\n",
        "\n",
        "    bb = open(bbfilename, \"r\")\n",
        "    bb_lines = bb.readlines()\n",
        "    bb.close()\n",
        "\n",
        "    return bb_lines, pngfiles"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDJASmq8ZrwJ",
        "colab_type": "text"
      },
      "source": [
        "**Parse Detections**\n",
        "\n",
        "We'll use this function in the main loop to wrangle the detections into the format we want to supply to our Particle Filter.\n",
        "\n",
        "Essentially it takes the name of png file, an img object and the list of bounding boxes as inputs.\n",
        "\n",
        "It then finds the correct record (if any) for that image in the bounding boxes list and converts the bounding box parameters into a format which we'll use for the rest of the program (it converts back to absolute pixel values).\n",
        "\n",
        "It returns a centre and a confidence value for the image supplied to it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQw8QFkqZxMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_detections(bboxes, pngfile, img):\n",
        "    # Sample Line: 400,-1,0.285417,0.241667,0.094792,0.483333,0.999797,-1,-1,-1\n",
        "    # Index, object type,\n",
        "    # x    - centre of bounding box (as fraction of image width\n",
        "    # y    - centre of bounding box (as fraction of image height\n",
        "    # w    - width of bounding box (as fraction of image width)\n",
        "    # h    - height of bounding box (as fraction of image height\n",
        "    # prob, _,_,_\n",
        "\n",
        "    # extract the frame index of the png file - \n",
        "    # use it to find the detections for that frame\n",
        "    index = int(re.findall(r'\\d+', pngfile)[-1])\n",
        "    imgh, imgw = img.shape[:2]\n",
        "\n",
        "    centre = np.zeros(shape=(2, 1))\n",
        "    P = 0.000001 # hack to avoid div by zero\n",
        "    for line in bboxes:\n",
        "        np_array = np.genfromtxt(io.StringIO(line), delimiter=\",\")\n",
        "        lineindex = int(np_array[0])\n",
        "\n",
        "        if lineindex == index:\n",
        "            centre = np_array[2:4]\n",
        "            P += np_array[6]\n",
        "            centre[0] *= imgw\n",
        "            centre[1] *= imgh\n",
        "\n",
        "            return centre, P\n",
        "\n",
        "    return centre, P\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4RRg22Hitb5",
        "colab_type": "text"
      },
      "source": [
        "#Particle Filter\n",
        "\n",
        "Look back over Particle Filter for an insight into how it is operating.\n",
        "\n",
        "The concept is:\n",
        "* For a low computational cost\n",
        "* Generate a set of sampled state update/prediction terms (particles)\n",
        "* Generate a measurement prediction from that state\n",
        "* Calculate the difference between the predicted measurement of the selected particles and the actual measurement\n",
        "* Adjust the state update/prediction particles (through a resampling stage to prevent degeneration of filter), and repeat....\n",
        "\n",
        "Done on Monte Carlo sampled particles.\n",
        "\n",
        "One key term to watch is how many particles to use for your specific tracking application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7dlyv0Nk7EU",
        "colab_type": "text"
      },
      "source": [
        "##Demo\n",
        "\n",
        "### Program Execution\n",
        "For each file:\n",
        "* get centre of detection (if any) and confidence from Yolo\n",
        "* feed Particle Filter with these values\n",
        "* Print internal Particles used by Particle Filter\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56tX2yoKlFiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "writer = None\n",
        "\n",
        "def demo_particle():\n",
        "    global writer\n",
        "    global numParticles\n",
        "    global velStd, xVel, yVel\n",
        "\n",
        "    # Initialise the filter with height of the frame, width of the frame\n",
        "    # and the number of particles\n",
        "    particleFilter = ParticleFilter(1920, 1080, numParticles)\n",
        "\n",
        "    bb_lines, pngfiles = get_pngs_and_boxes()\n",
        "\n",
        "    for pngfile in pngfiles:\n",
        "        #print(\"handling ..\" + os.path.basename(pngfile))\n",
        "        img = cv2.imread(pngfile)\n",
        "\n",
        "        # Derive meas-var from yolo confidence level in detection\n",
        "        raw_centre, conf = parse_detections(bb_lines, pngfile, img)\n",
        "\n",
        "        # Crudely derive meas-var.  If yolo is confident we want a small\n",
        "        # uncertainty. If yolo isn't confident, translate to\n",
        "        # a large uncertainty.\n",
        "        if(conf > 0.50):\n",
        "            lStd = velStd\n",
        "        else:\n",
        "            lStd = 1\n",
        "\n",
        "        # update weights of particles based on measure\n",
        "        particleFilter.update(raw_centre.item(0), raw_centre.item(1))\n",
        "\n",
        "        # Pretty print particles\n",
        "        for i in range(0, numParticles):\n",
        "                x_part, y_part = particleFilter.returnParticlesCoordinates(i)\n",
        "                cv2.circle(img, (x_part, y_part), 10, (0,255,0),-1)\n",
        "\n",
        "        # Resize and show the image\n",
        "        img2 = cv2.resize(img, (int(img.shape[1]/4), int(img.shape[0]/4)))\n",
        "\n",
        "        # update model - using 5 pixels in x and y and adjusting model variance\n",
        "        # depending on yolo confidence\n",
        "        particleFilter.predict(x_velocity=xVel,y_velocity=yVel, std=lStd)\n",
        "\n",
        "        # estimate the position of the point, based on particle weights\n",
        "        x_est, y_est, _, _ = particleFilter.estimate()\n",
        "\n",
        "        #The resampling draws particles from the current set with a \n",
        "        # probability given by the current weights.\n",
        "        # The new set is an approximation of the distribution which represents the state\n",
        "        # of the particles at time t. \n",
        "        # The resampling solves this problem: after some iterations of the algorithm\n",
        "        # some particles are useless because they do not represent the point \n",
        "        #position anymore, eventually they will be too far away from the real position.\n",
        "        #The resample function removes useless particles and keep the\n",
        "        #useful ones.\n",
        "        particleFilter.resample()\n",
        "\n",
        "        # Build a frame of our output video\n",
        "        if writer is None:\n",
        "            # Initialize our video writer\n",
        "            fourcc = cv2.VideoWriter_fourcc(*'VP80')\n",
        "            writer = cv2.VideoWriter('video.webm', fourcc, 30, (img2.shape[1], img2.shape[0]), True)\n",
        "\n",
        "        # Write the output frame to disk\n",
        "        writer.write(img2)\n",
        "\n",
        "    # Release the file pointers\n",
        "    writer.release()\n",
        "\n",
        "demo_particle()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fZJvZXeY15y",
        "colab_type": "text"
      },
      "source": [
        "**Video**\n",
        "\n",
        "Thia code plays the video we just made.\n",
        "\n",
        "The Particles being used by the Filter dot in red.\n",
        "\n",
        "As you can see, Particle Filtering has a role to play in predicting a reasonable guess for where the object might be while it is off-camera - it should be better than Kalman for non-linear motion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxvG8It3RlWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set this to 1 if video display\n",
        "# is not working - works with chrome and firefox, not with safari\n",
        "videoBodge = 0\n",
        "\n",
        "def arrayShow (imageArray):\n",
        "    ret, png = cv2.imencode('.png', imageArray)\n",
        "    encoded = base64.b64encode(png)\n",
        "    return Image(data=encoded.decode('ascii'))\n",
        "\n",
        "if(videoBodge == 0):\n",
        "    from IPython.display import HTML\n",
        "    from base64 import b64encode\n",
        "    webm = open('video.webm','rb').read()\n",
        "    data_url = \"data:video/webm;base64,\" + b64encode(webm).decode()\n",
        "else:\n",
        "    video = cv2.VideoCapture(\"video.webm\")\n",
        "    while(video.isOpened()):\n",
        "        clear_output(wait=True)\n",
        "        ret, frame = video.read()\n",
        "        if(ret == False):\n",
        "          break\n",
        "        lines, columns, _ =  frame.shape\n",
        "        img = arrayShow(frame)\n",
        "        display(img)\n",
        "        time.sleep(1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ3zQNbG86Dq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display Video\n",
        "HTML(\"\"\"\n",
        "<video width=200 controls>\n",
        "      <source src=\"%s\" type=\"video/webm\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH4IGJkZlO_u",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "## Exercises\n",
        "**Exercise 1**\n",
        "Simulate occluding the object being detected - for example, only supply every second measurement update to the Particle Filter and observe the Filter behaviour.\n",
        "\n",
        "**Exercise 2**\n",
        "Simply multiply and divide the number of particles and observe how that affects the Particle Filter's predictions.\n",
        "\n",
        "\n",
        "## Takeaways\n",
        "1. You've seen a Particle Filter used for single object tracking\n",
        "2. You've seen that a Particle Filter can help deal with occlusions - i.e. in this example the object being tracked disappeared for a few frames and the Particle Filter continued to predict motion for it based on its model.\n",
        "3. You've seen that you probably need to tune the Filter to get it working for a particular application.\n",
        "\n",
        "## Next Steps\n",
        "1. We've seen a set of tracking filters, from the same 'Kalman' family.  Now, we'll look at how to track multiple objects in a scene and then we'll look at other tracking techiques not in the 'Kalman' family such as Optical Flow and CNN Feature based trackers."
      ]
    }
  ]
}