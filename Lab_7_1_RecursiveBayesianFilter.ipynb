{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_7_1_RecursiveBayesianFilter.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baqq6FcdVNTD",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Recursive Bayes Filter - Lab 7.1\n",
        "\n",
        "## Recap\n",
        "\n",
        "This is the Lab on using a Recursive Bayes Filter in CE6003's Object Tracking. You should complete the tasks in this lab as part of the Bayes section of the lesson.\n",
        "\n",
        "Please remember this lab must be completed before taking the quiz at the end of this lesson.\n",
        "\n",
        "First, if we haven't already done so, we need to clone the various images and resources needed to run these labs into our workspace.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWrzzv1DVwbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/EmdaloTechnologies/CE6003.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXyxCpyTVzlu",
        "colab_type": "text"
      },
      "source": [
        "This program demonstrates a very simple 'tracking' mechanism - derived from a Bayesian approach.\n",
        "\n",
        "It improves the estimate for the position of a static object (e.g. a car) as new position estimates (e.g. from a GPS system) arrives.\n",
        "\n",
        "It illustrates the shape of the (x,y) measurements arriving to the algorithm as a histogram of x, a histogram of y - each with mean and variance, a scatter plot of the 'measurements' arriving and as a Co-variance of x and y.  These are terms and concepts we'll use throughout the lessons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9--h9Em3xMUc",
        "colab_type": "text"
      },
      "source": [
        "First, lets import our typical libaries; numpy, scipy, math for matrix maths, matplotlib, mplot3d for plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5UKegQGXK-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Our imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.cm as cm\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import scipy.stats as stats\n",
        "import os\n",
        "import math\n",
        "from IPython import display\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6_-9iCHXXcB",
        "colab_type": "text"
      },
      "source": [
        "**Program Structure**\n",
        "\n",
        "After visualising the input data, the program simply\n",
        "loops for 'iterations' each time refining its position estimate based on an array of 'measurements', fed to it one at a time.\n",
        "\n",
        "the actual position is 4,5 but the program doesn't know that\n",
        "- instead it build a region where it becomes increasing confident that, for each update, the car should be (from a series of estimates centred around 4,5 where the estimation error is normal or gaussian)\n",
        "\n",
        "In this example, this assumption is valid as the estimation error is defined as a gaussian around where the car actually is.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGqewbSnyV_4",
        "colab_type": "text"
      },
      "source": [
        "**Major Model Variables**\n",
        "\n",
        "The major variables that control the model are:\n",
        "\n",
        "**iterations:** how may times to run the model\n",
        "\n",
        "**gridsize:** the size of the x-y grid to place the car in\n",
        "\n",
        "**actual_pos:** where the car actually is on the grid\n",
        "\n",
        "**variance:** the variance in the measurements (both in x and y)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u7no5GyXyFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# major model variables\n",
        "iterations = 50         # number of iterations to run the model\n",
        "gridsize=(40,40)        # the size of the grid containing our car\n",
        "actual_pos = (4,5)      # the actual position of the car in the grid\n",
        "\n",
        "variance = (2,2)        # the variances (in x and y) of the measurement estimates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0hZJPc5X4Fb",
        "colab_type": "text"
      },
      "source": [
        "**Estimates**\n",
        "\n",
        "Create the estimates up front - ensure they are centred around actual_pos and the estimates vary in a gaussianÂ manner with the variance defined above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8CmN_6oX8Gd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimates = np.zeros((iterations,2), dtype=float)\n",
        "estimates[:,0] = np.random.normal(actual_pos[0], variance[0], iterations)\n",
        "estimates[:,1] = np.random.normal(actual_pos[1], variance[1], iterations)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTWEPuAtYGvZ",
        "colab_type": "text"
      },
      "source": [
        "# Visualising the input data\n",
        "\n",
        "This shows visualisations of the data. It illustrates the estimates as a histogram in x with mean and variance, a histogram in y with mean and variance and a scatter-plot showing the estimates centred around the 'actual' value of x,y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7leWxDXYUYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create 2 x 2 sub plots\n",
        "\n",
        "gs = gridspec.GridSpec(2,2)\n",
        "plt.figure(figsize=np.array([210,297]) / 25.4)\n",
        "ax = plt.subplot(gs[1, 0]) # row 1, col 0\n",
        "ax.set_title('X Position Estimates Histogram')\n",
        "ax.hist(estimates[:,0],density=True,bins=30)\n",
        "ax.set(xlabel='Weight', ylabel = 'Probability')\n",
        "\n",
        "ay = plt.subplot(gs[1,1]) # row 1, col 1\n",
        "ay.set_title('Y Position Estimates Histogram')\n",
        "ay.hist(estimates[:,1],density=True,bins=30)\n",
        "ay.set(xlabel='Weight', ylabel = 'Probability')\n",
        "\n",
        "sc = plt.subplot(gs[0, :]) # row 0, span all columns\n",
        "sc.scatter(actual_pos[0], actual_pos[1],color='red', s=150)\n",
        "sc.scatter(estimates[:,0],estimates[:,1],color='blue')\n",
        "sc.set_title('Scatter-Plot of Position Estimates')\n",
        "sc.set(xlabel = 'X', ylabel='Y')\n",
        "plt.show()\n",
        "plt.close('all')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0aLfvSyE5W2",
        "colab_type": "text"
      },
      "source": [
        "**Position Estimates**\n",
        "\n",
        "Here, we're forming an intuition about how the estimates will arrive, over time, to our Recursive Bayesian Filter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A6cbexlYloe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Intuition as to how data will arrive into the algorithm\n",
        "min_x = min(estimates[:,0])\n",
        "max_x = max(estimates[:,0])\n",
        "min_y = min(estimates[:,1])\n",
        "max_y = max(estimates[:,1])\n",
        "fig = plt.figure()\n",
        "\n",
        "plt.grid(True)\n",
        "plt.scatter(actual_pos[0], actual_pos[1],color='red', s=150)\n",
        "scat = plt.scatter(estimates[:0,0],estimates[:0,1],color='blue')\n",
        "plt.title('Scatter-Plot of Position Estimates')\n",
        "plt.xlabel = 'X'\n",
        "plt.ylabel='Y'\n",
        "plt.xlim(min_x-2, max_x+2)\n",
        "plt.ylim(min_y-2, max_y+2)\n",
        "\n",
        "ax = fig.gca()\n",
        "\n",
        "display.clear_output(wait=True)\n",
        "\n",
        "for i in range(iterations):\n",
        "        plt.grid(True)\n",
        "        plt.scatter(actual_pos[0], actual_pos[1],color='red', s=150)\n",
        "        plt.scatter(estimates[:i,0],estimates[:i,1],color='blue')\n",
        "        plt.title('Scatter-Plot of Position Estimates')\n",
        "        plt.xlabel = 'X'\n",
        "        plt.ylabel='Y'\n",
        "        plt.xlim(min_x-2, max_x+2)\n",
        "        plt.ylim(min_y-2, max_y+2)\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2kCl-DoFWcl",
        "colab_type": "text"
      },
      "source": [
        "**Covariance**\n",
        "\n",
        "In this plot, we're going to visualise the covariance in the x and y terms of our estimates.\n",
        "\n",
        "One key takeaway from this - before we run the estimates through our RBF is to note the spread of the variance.  We'll be hoping to show that the RBF reduces this variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-IK6O6eYyq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize the probabilities of the estimates\n",
        "# as a 3D height map\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.gca(projection='3d')\n",
        "hist, xedges, yedges = np.histogram2d(estimates[:,0], \\\n",
        "                                      estimates[:,1], bins=(30,30))\n",
        "X, Y = np.meshgrid(xedges[:-1], yedges[:-1])\n",
        "pos = np.empty(X.shape + (2,))\n",
        "pos[:, :, 0] = X; pos[:, :, 1] = Y\n",
        "rv = stats.multivariate_normal(actual_pos, \\\n",
        "                               [[variance[0], 0], [0, variance[1]]])\n",
        "surf = ax.plot_surface(X,Y, rv.pdf(pos), \\\n",
        "                       cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Probability')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxgv6jDSY2Vw",
        "colab_type": "text"
      },
      "source": [
        "**Recursive Bayesian Filter (RBF)**\n",
        "\n",
        "Now, we are going to iteratively solve:\n",
        "\n",
        "$$\\displaystyle P(A\\mid B)={\\frac {P(B\\mid A)P(A)}{P(B)}} $$\n",
        "\n",
        "where A and B are events (and P(B) is non-zero)\n",
        "\n",
        "$P(A\\mid B)$ is a *conditional probability*:\n",
        "\n",
        "       what is the likelihood of seeing A given B is true\n",
        "\n",
        "$P(B\\mid  A)$ is also a *conditional probability*:\n",
        "\n",
        "       what is the likelihood of B given a is true\n",
        "\n",
        "P(A) and P(B) are the probabilities of seeing A and B independently of each other.\n",
        "\n",
        "Or, put another way:\n",
        "\n",
        "For each point in the grid, on the arrival of a new estimate, the probability that it is the 'right' point is affected by its old probability and how likely it is that it can account for the new estimate.\n",
        "\n",
        "  $\\displaystyle{post = \\frac{(prior * likelihood)}{normalisation}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PjeuVssGScR",
        "colab_type": "text"
      },
      "source": [
        "**Initialisation of Terms**\n",
        "\n",
        "We're defining a prior term where we create a grid (in the shape of gridsize, one point for each position in the grid) and we initialise that to $\\frac{1}{num-gridpoints}$, i.e. all squares in the grid are equally likely to be the correct square.\n",
        "\n",
        "We create a post with the same shape as the prior and initialise it to the prior.\n",
        "\n",
        "Now we want to handle the uncertainty in our state.  Effectively we're going to treat this as deciding how confident we are in our position relative to our measurement.\n",
        "\n",
        "We create a co-variance term as a 2 by 2 matrix, looking like this:\n",
        "\n",
        "$K = \\begin{bmatrix}4 & 0 \\\\ 0 & 4\\end{bmatrix}$\n",
        "\n",
        "So, the first term only affects the x-term of our estimates and the second term only affects the y-term of our estimates and each term represents an variance of 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMeOEIuHbimw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a prior matrix with one point for every point\n",
        "# on the grid.\n",
        "prior = np.zeros(shape=gridsize)\n",
        "# nothing is known at this stage so all squares are equally likely\n",
        "# initialise all with same probability (1 / num squares)\n",
        "prior = prior + 1/np.sum(gridsize)\n",
        "\n",
        "# Create a post matrix\n",
        "post = np.zeros(shape=gridsize)\n",
        "# set to same value as priors for now\n",
        "post = prior\n",
        "\n",
        "# define a covariance matrix K for making a 2-D Gaussian variance\n",
        "K = [ (4, 0), (0, 4)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LubDhlssImdT",
        "colab_type": "text"
      },
      "source": [
        "**Exercise 1**\n",
        "\n",
        "Adjust the measurement co-variance in the K term above and monitor its affect in the animation below. For example, multiply by 10 and divide by 10.\n",
        "\n",
        "**Exercise 2**\n",
        "Adjust the *variance* term $(2,2)$ above and monitor its affect on the animation below.\n",
        "\n",
        "**Insight**\n",
        "These two exercises illustrate the key relationship in the RCF - the impact of the variance of position estimate vs measurement variance on the behaviour of the model.\n",
        "\n",
        "**Exercise 3**\n",
        "Generate the position estimates differently.  Instead of 200 estimates around a single mean, create 100 estimates around one mean (e.g. 5,4) and append another 100 estimates around a second mean (e.g. 4,5).  Observe how the filter copes with this.\n",
        "\n",
        "**Insight**\n",
        "The insight here is that we can see a route where we can use the RCF (or a variation thereof) to take account of a moving object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "velFQ0nlKKqt",
        "colab_type": "text"
      },
      "source": [
        "**Grid Adjustment**\n",
        "\n",
        "To make the grid more generic, we leave the grid based on the size of grid in gridsize, but we set the range and interval of the grid to something sensible, for instance we could say that our grid consists of 40 values in x and 40 values in y; and that the values start at half the value of the x-value we are estimating and the x values end at 1.5 times the x-value we are estimating and that the interval is linear between these two values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDkT5tAhKGMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a grid (defined as gridsize), initialise all points\n",
        "x_range = np.zeros(gridsize[0])\n",
        "y_range = np.zeros(gridsize[1])\n",
        "\n",
        "# initialise x_range with values from x0 .. X .. xn such that \n",
        "# actual_pos is in the middle of the grid and the grid is\n",
        "# scaled in a reasonable manner\n",
        "min_x = actual_pos[0] - (actual_pos[0])/2\n",
        "max_x = actual_pos[0] + (actual_pos[0])/2\n",
        "step_x = (max_x - min_x) / gridsize[0]\n",
        "for i in range(gridsize[0]):\n",
        "        x_range[i] = min_x + i*step_x\n",
        "\n",
        "# initialise y_range with values from y0 .. Y .. yn such \n",
        "# that actual_pos is in the middle and the grid is scaled in\n",
        "# a reasonable manner\n",
        "min_y = actual_pos[1] - (actual_pos[1])/2\n",
        "max_y = actual_pos[1] + (actual_pos[1])/2\n",
        "step_y = (max_y - min_y) / gridsize[1]\n",
        "for i in range(gridsize[1]):\n",
        "        y_range[i] = min_y + i*step_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejKZBFjUIgv_",
        "colab_type": "text"
      },
      "source": [
        "**Main Loop of Recursive Bayesian Filter**\n",
        "\n",
        "On each iteration, this visits each square on the grid and determines how likely that this point is to be 'correct', given the new estimate.\n",
        "\n",
        "We multiply that likelihood term by the square's prior - effectively a measure of how 'confident' the square was that it was the 'correct' square.\n",
        "\n",
        "We crudely normalise the $(prior \\times likelihood)$ term, by normalising to 1. Simply sum all $(prior \\times likelihood)$ terms in the grid and divide each $(prior \\times likelihood)$ term by the sum.  \n",
        "\n",
        "Re-summing will now equal to 1 - so we've crudely converted back into a probability distribution - effectively now we can recurse infinitely.\n",
        "\n",
        "Compute likelihood of receiving a measurement at using the normal distribution function\n",
        "\n",
        "${P(x|\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^\\frac{-(x - \\mu)^2}{2\\sigma^2}}$\n",
        "\n",
        "We'll compute the likelihood of x and y separately, deriving our variance from the K matrix and multiply.\n",
        "\n",
        "We'll approximate the shape of the Gaussian by using $abs(x-\\mu)$ instead of $\\frac{(x-\\mu)^2}{2\\sigma^2}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbV-dAkSbrhK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "display.clear_output(wait=True)\n",
        "\n",
        "#\n",
        "# On each iteration:\n",
        "#       For each point:\n",
        "#               How likely is it that this point can 'explain' the new estimate\n",
        "#               Multiply that by how confident the point was\n",
        "#\n",
        "#       Convert all new values to a prob dist by ensuring they total to 1.\n",
        "#\n",
        "for iteration in range(iterations):\n",
        "        prior = post    # store the (old) post to the prior\n",
        "        m = 0 * prior   # m is our working area and starts at zero\n",
        "\n",
        "        # likelihood algorithm\n",
        "        #       look at each location.\n",
        "        #       assume that location is the correct location\n",
        "        #       get the likelihood of the point accounting for the\n",
        "        #       estimate  assuming 2-D gaussian noise\n",
        "        for i in range(gridsize[0]):\n",
        "                for j in range(gridsize[1]):\n",
        "                        # compute likelihood\n",
        "\n",
        "                        # this represents where we 'think' we are\n",
        "                        pos = (x_range[i], y_range[j])\n",
        "\n",
        "                        likelihood = ( (1 / (np.sqrt((2*(np.pi)**2)))) * np.linalg.det(K) * np.exp(-(((estimates[iteration]-pos)**2)/2*(np.linalg.det(K)))))\n",
        "                             #( (1 / (np.sqrt((2*(np.pi)**2)))) * \\\n",
        "                             #np.linalg.det(K) * \\\n",
        "                             #np.exp(-(((estimates[iteration]-pos)**2)/2*(K))))))\n",
        "                             #np.exp(-(np.absolute(estimates[iteration]-pos))))\n",
        "                            \n",
        "                        # how likely we are to see this estimate\n",
        "                        m[i,j] = likelihood[0]*likelihood[1]\n",
        "\n",
        "                        ## combine this likelihood with prior confidence\n",
        "                        m[i,j] = m[i,j] * prior[i,j]\n",
        "\n",
        "        # normalise this distribution to make it\n",
        "        # a probability distribution\n",
        "        post = m / np.sum(m)\n",
        "        \n",
        "        # Pretty pictures - plot Post\n",
        "        fig = plt.figure()\n",
        "        ax = fig.gca(projection='3d')\n",
        "        x = x_range\n",
        "        y = y_range\n",
        "\n",
        "        X, Y = np.meshgrid(x, y)\n",
        "        surf = ax.plot_surface(X,Y, post, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
        "        ax.set_xlabel('X')\n",
        "        ax.set_ylabel('Y')\n",
        "        ax.set_zlabel('Probability')\n",
        "        ax.set_zlim(0,0.8)\n",
        "\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)\n",
        "\n",
        "plt.close('all')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNph9HbP-Tnh",
        "colab_type": "text"
      },
      "source": [
        "*Conclusion*\n",
        "\n",
        "The key takeaways are:\n",
        "\n",
        "1. We have introduced the concept that we are treating our position and our estimates as probability terms - defined by their mean and variance (standard deviation squared).\n",
        "2. We have the developed the concept of using Bayes Theorem to successively improve an estimate\n",
        "3. We have applied that insight to locating a static object, iteratively using uncertain initial position and uncertain measurements.\n",
        "4. We have gained the insight that one or two 'poor' measurements do not significantly adjust our position confidence.\n",
        "5. We have introduced three key terms - $posterior$ or $post$ $prior$ and $measurement$, where each term is essentially conceived as a mean and variance about that mean of that term.  We're treating $prior$ as essentially confidence in position before $measurement$ arrival, $measurement$ as essentially a confidence in a measurement, and $post$ as essentially a new confidence in position after $measurement$.\n",
        "\n",
        "*Next Steps*\n",
        "1. Kalman Filters\n",
        "2. Particle Filters"
      ]
    }
  ]
}